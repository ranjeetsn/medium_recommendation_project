{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from datetime import datetime\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MONGO_CONNECTION_STRING = os.getenv(\"MONGO_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "URI = MONGO_CONNECTION_STRING\n",
    "DB_NAME = \"medium_database\"\n",
    "COLLECTION_NAME = \"writer_information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the desired collection.\n",
    "client = MongoClient(URI, server_api=ServerApi('1'))\n",
    "db = client[DB_NAME]\n",
    "writer_info = db[COLLECTION_NAME]\n",
    "\n",
    "# Prepare the raw dataset.\n",
    "dataset = {\"followers_count\": [],\n",
    "            \"publication_following_count\": [],\n",
    "            \"has_twitter_username\": [],  # convert from string to bool\n",
    "            \"is_writer_program_enrolled\": [],\n",
    "            \"allow_notes\": [],\n",
    "            \"medium_member_at\": [],  # need to calculate the time later\n",
    "            \"is_book_author\": [],\n",
    "            \"title\": [],  # need to do NLP later\n",
    "            \"subtitle\": [],  # need to do NLP later\n",
    "            #\"tags\": [],\n",
    "            #\"topics\": [],\n",
    "            \"word_count\": [],\n",
    "            \"reading_time\": [],\n",
    "            \"is_series\": [],\n",
    "            \"is_shortform\": [],\n",
    "            \"top_highlight\": [],  # need to do NLP later\n",
    "            \"content\": [],  # need to do NLP later\n",
    "            \"claps\": [],\n",
    "            \"voters\": [],\n",
    "            \"responses_count\": []}\n",
    "\n",
    "for writer in writer_info.find():\n",
    "    for article in writer[\"top_articles\"]:\n",
    "        dataset[\"followers_count\"].append(writer.get(\"followers_count\", 0))\n",
    "        dataset[\"publication_following_count\"].append(writer.get(\"publication_following_count\", 0))\n",
    "        dataset[\"has_twitter_username\"].append(writer.get(\"twitter_username\", \"\").strip() != \"\")\n",
    "        dataset[\"is_writer_program_enrolled\"].append(writer.get(\"is_writer_program_enrolled\", False))\n",
    "        dataset[\"allow_notes\"].append(writer.get(\"allow_notes\", False))\n",
    "        dataset[\"medium_member_at\"].append(writer.get(\"medium_member_at\", \"\").strip())\n",
    "        dataset[\"is_book_author\"].append(writer.get(\"is_book_author\", False))\n",
    "        dataset[\"title\"].append(article.get(\"title\", \"\").strip())\n",
    "        dataset[\"subtitle\"].append(article.get(\"subtitle\", \"\").strip())\n",
    "        dataset[\"word_count\"].append(article.get(\"word_count\", 0))\n",
    "        dataset[\"reading_time\"].append(article.get(\"reading_time\", 0))\n",
    "        dataset[\"is_series\"].append(article.get(\"is_series\", False))\n",
    "        dataset[\"is_shortform\"].append(article.get(\"is_shortform\", False))\n",
    "        dataset[\"top_highlight\"].append(article.get(\"top_highlight\", \"\").strip())\n",
    "        dataset[\"claps\"].append(article.get(\"claps\", 0))\n",
    "        dataset[\"voters\"].append(article.get(\"voters\", 0))\n",
    "        dataset[\"responses_count\"].append(article.get(\"responses_count\", 0))\n",
    "\n",
    "        if \"content\" in article:\n",
    "            dataset[\"content\"].append(article[\"content\"].get(\"content\", \"\").strip())\n",
    "        else:\n",
    "            dataset[\"content\"].append(\"\")\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert medium_member_at to medium_usage_time in days.\n",
    "\n",
    "medium_usage_time = []\n",
    "current_date = datetime.strptime(\"2024-02-24 16:45:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "for medium_member_at in dataset[\"medium_member_at\"]:\n",
    "    if medium_member_at == \"\":\n",
    "        medium_usage_time.append(0)\n",
    "        continue\n",
    "\n",
    "    date = datetime.strptime(medium_member_at, \"%Y-%m-%d %H:%M:%S\")\n",
    "    medium_usage_time.append((current_date - date).days)\n",
    "dataset[\"medium_usage_time\"] = medium_usage_time\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do tfidf to all text fields.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def sanitize(texts):\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        text = t.lower()\n",
    "        text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', text)\n",
    "        text = re.sub('\\\\s+', ' ', text)\n",
    "        text = nlp(text)\n",
    "        tokens = [words.lemma_ for words in text if not words.is_stop and len(words) >= 2]\n",
    "        result.append(\" \".join(tokens))\n",
    "    return result\n",
    "\n",
    "\n",
    "def vectorize(col_name, max_num):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "    contents = sanitize(dataset[col_name].values)\n",
    "    tfidf_matrix = vectorizer.fit_transform(contents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_sum = np.sum(tfidf_matrix, axis=0)\n",
    "    tfidf_sum_array = np.squeeze(np.asarray(tfidf_sum))\n",
    "    tfidf_mapping = dict(zip(feature_names, tfidf_sum_array))\n",
    "    sorted_tfidf = sorted(tfidf_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = sorted_tfidf[:max_num]\n",
    "    indices = [np.where(feature_names == word)[0][0] if word in feature_names else None for word, _ in top_words]\n",
    "    return tfidf_matrix.toarray()[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 219)\n"
     ]
    }
   ],
   "source": [
    "def update_dataset_with_vector(dataset, col_name, max_num):\n",
    "    dataset_copy = dataset.copy(deep=True)\n",
    "    vector = vectorize(col_name, max_num)\n",
    "    n_cols = vector.shape[1]\n",
    "    for i in range(n_cols):\n",
    "        dataset_copy[f\"{col_name}_{i}\"] = vector[:, i]\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "dataset_final = update_dataset_with_vector(dataset, \"title\", 50)\n",
    "dataset_final = update_dataset_with_vector(dataset_final, \"subtitle\", 50)\n",
    "dataset_final = update_dataset_with_vector(dataset_final, \"top_highlight\", 50)\n",
    "dataset_final = update_dataset_with_vector(dataset_final, \"content\", 50)\n",
    "print(dataset_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: 0.07105536043775662\n",
      "Random Forest R2: 0.07725905911298092\n",
      "Linear Regression R2: -0.07663117680937925\n",
      "Random Forest R2: -0.17260835995974344\n",
      "Linear Regression R2: -0.26205881868656955\n",
      "Random Forest R2: -2.068504375642978\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DataType(Enum):\n",
    "    TEXT_ONLY = 0\n",
    "    TEXT_EXCLUSIVE = 1\n",
    "    ALL_DATA = 2\n",
    "\n",
    "\n",
    "def get_X(X, data_type=DataType.ALL_DATA):\n",
    "    non_text_cols = [\"followers_count\",\n",
    "                    \"publication_following_count\",\n",
    "                    \"has_twitter_username\",\n",
    "                    \"is_writer_program_enrolled\",\n",
    "                    \"allow_notes\",\n",
    "                    \"medium_usage_time\",\n",
    "                    \"is_book_author\",\n",
    "                    \"word_count\",\n",
    "                    \"reading_time\",\n",
    "                    \"is_series\",\n",
    "                    \"is_shortform\"]\n",
    "    X_new = X.copy(deep=True)\n",
    "    if data_type == DataType.TEXT_ONLY:\n",
    "        X_new = X_new.drop(columns=non_text_cols)\n",
    "    elif data_type == DataType.TEXT_EXCLUSIVE:\n",
    "        X_new = X_new[non_text_cols]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# Try linear regression\n",
    "def fit_linear_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    #y_pred = model.predict(X_test)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f\"Linear Regression R2: {score}\")\n",
    "\n",
    "\n",
    "def fit_random_forest(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    model = RandomForestRegressor(n_estimators=50, max_depth=10,ccp_alpha = 0.1, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    print(f\"Random Forest R2: {score}\")\n",
    "\n",
    "\n",
    "col_to_exclude = [\"medium_member_at\", \"title\", \"subtitle\", \"top_highlight\", \"content\", \"claps\", \"voters\", \"responses_count\"]\n",
    "X = dataset_final.drop(columns=col_to_exclude)\n",
    "y_claps = dataset_final[\"claps\"]\n",
    "y_voters = dataset_final[\"voters\"]\n",
    "y_responses_count = dataset_final[\"responses_count\"]\n",
    "fit_linear_regression(X, y_claps)\n",
    "fit_random_forest(X, y_claps)\n",
    "fit_linear_regression(X, y_voters)\n",
    "fit_random_forest(X, y_voters)\n",
    "fit_linear_regression(X, y_responses_count)\n",
    "fit_random_forest(X, y_responses_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: -0.09344272348701121\n",
      "Linear Regression R2: -1.3020934484506022\n",
      "Linear Regression R2: -1.0231194847982348\n",
      "Random Forest R2: -2.09958169609855\n",
      "Random Forest R2: 0.022014609427925214\n",
      "Random Forest R2: 0.16549284337926506\n",
      "Linear Regression R2: -0.18901840672510417\n",
      "Linear Regression R2: -9.325537583965108\n",
      "Linear Regression R2: -0.6500885508936134\n",
      "Random Forest R2: -0.7132404489272217\n",
      "Random Forest R2: 0.29584197179880234\n",
      "Random Forest R2: 0.016305983264450208\n",
      "Linear Regression R2: -0.022927453453716273\n",
      "Linear Regression R2: -2.0220158583869976\n",
      "Linear Regression R2: -0.5086904654329325\n",
      "Random Forest R2: -0.8288271485451644\n",
      "Random Forest R2: 0.011168421785156424\n",
      "Random Forest R2: -0.7365404262372661\n"
     ]
    }
   ],
   "source": [
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE), y_claps)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y_claps)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y_claps)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y_claps)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y_claps)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y_claps)\n",
    "\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE),y_voters)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y_voters)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y_voters)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y_voters)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y_voters)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y_voters)\n",
    "\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE),y_responses_count)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y_responses_count)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y_responses_count)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y_responses_count)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y_responses_count)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y_responses_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongodb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
